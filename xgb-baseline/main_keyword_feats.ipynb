{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, auc, f1_score\n",
    "\n",
    "from urllib.parse import quote, unquote, urlparse\n",
    "from xgboost import XGBClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 禁止hash随机化\n",
    "\n",
    "\n",
    "set_seed(2022)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66be32e3902a49aca64d31271246bb98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33219\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "train_files = glob.glob('../data/train/*.csv')\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "for filepath in tqdm(train_files):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df_train = pd.concat([df_train, df]).reset_index(drop=True)\n",
    "\n",
    "df_train.fillna('__NaN__', inplace=True)\n",
    "\n",
    "# 强迫症发作..\n",
    "df_train = df_train.rename(columns={'lable': 'label'})\n",
    "df_train\n",
    "print(len(df_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(37219, 7)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label\n",
    "# 0. 白\n",
    "# 1. SQL 注入\n",
    "# 2. 目录历遍\n",
    "# 3. 远程代码执行\n",
    "# 4. 命令执行\n",
    "# 5. XSS 跨站脚本\n",
    "\n",
    "# %%\n",
    "\n",
    "df_test = pd.read_csv('../data/test/test.csv')\n",
    "df_test.fillna('__NaN__', inplace=True)\n",
    "df_test\n",
    "\n",
    "# %%\n",
    "\n",
    "df = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "print('-------------------提取body, url的len特征 start...------------------------')\n",
    "#df['body_len'] = df['body'].apply(lambda x: len(list(x)))\n",
    "#df['url_len'] = df['url'].apply(lambda x: len(list(x)))\n",
    "print('-------------------提取body. url的len特征 end...------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('-------------------提取攻击关键词特征 start...------------------------')\n",
    "XSS_key_words_high = ['<scrip', '</script', '<iframe', '</iframe', 'response',\n",
    "                 'write(','eval(','prompt(','alert(','javascript;','document','cookie']\n",
    "XSS_key_words_middle = ['onclick=','onerror=','<!--','-->','<base','</base>>','location','hash','window','name','<form','</form']\n",
    "XSS_key_words_low = ['echo','print','href=','sleep']\n",
    "XSS_key_words = XSS_key_words_high + XSS_key_words_middle + XSS_key_words_low\n",
    "\n",
    "SQL_key_words_high = ['and','or','xp_','substr','utl_','benchmark','shutdown','@@version','mformation_schema','hex(']\n",
    "SQL_key_words_middle = ['select','if(','union','group','by','--','count(','/**/','char(','drop','delete','concat','orderby',\n",
    "                        'case when','assic(','exec(','length']\n",
    "SQL_key_words_low = ['and','or','like','from','insert', 'update','create','else', 'exist','table' ,'database','where','sleep','mid',\n",
    "                    'updatexml(','null','sqlmap','md5(','floorm','rand','cast','dual','fetch','print','declare','cursor',\n",
    "                    'extractvalue(','upperjoin','exec','innier','convert','distinct']\n",
    "SQL_key_words = SQL_key_words_high+SQL_key_words_middle+SQL_key_words_low"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Have_XSS_attack_feat(x):\n",
    "    for kw in XSS_key_words:\n",
    "        if kw in x:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def Nums_XSS_attack_feat(x):\n",
    "    count = 0\n",
    "    for kw in XSS_key_words:\n",
    "        if kw in x:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "#df['Have_XSS_attack_feat_body'] = df['body'].apply(lambda x: Have_XSS_attack_feat(x))\n",
    "df['Nums_XSS_attack_feat_body'] = df['body'].apply(lambda x: Nums_XSS_attack_feat(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def Have_SQL_attack_feat(x):\n",
    "    for kw in SQL_key_words:\n",
    "        if kw in x:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def Nums_SQL_attack_feat(x):\n",
    "    count = 0\n",
    "    for kw in SQL_key_words:\n",
    "        if kw in x:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "#df['Have_SQL_attack_feat_body'] = df['body'].apply(lambda x: Have_SQL_attack_feat(x))\n",
    "df['Nums_SQL_attack_feat_body'] = df['body'].apply(lambda x: Nums_SQL_attack_feat(x))\n",
    "\n",
    "print('-------------------提取攻击关键词特征 end...------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------提取特殊字符特征 start...------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------提取特殊字符特征 start...------------------------')\n",
    "special_chars = ['\\\\', '/', '!', '%', '#', '&', '?', '+', '-', '*', '=', '{', '}', '(', ')',\n",
    "                '[', ']', '@', '^', '$', '<', '>', ':', ';', '~']\n",
    "\n",
    "def Nums_special_char_feat(x):\n",
    "    count = 0\n",
    "    for kw in special_chars:\n",
    "        if kw in x:\n",
    "            count += 1\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "df['Nums_special_char_feat_body'] = df['body'].apply(lambda x: Nums_special_char_feat(x))\n",
    "df['Nums_special_char_feat_url'] = df['url'].apply(lambda x: Nums_special_char_feat(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('-------------------提取特殊字符特征 end...------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "0     14148\n10    10619\n11     3757\n12     1867\n9      1768\n7       757\n25      750\n8       713\n13      642\n14      347\n5       280\n6       265\n15      251\n4       233\n16      166\n3       113\n17      102\n2        74\n23       65\n24       59\n22       52\n19       50\n1        40\n20       39\n21       34\n18       28\nName: Nums_special_char_feat_body, dtype: int64"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "%\n",
      "&\n",
      "?\n",
      "-\n",
      "=\n",
      "(\n",
      ")\n",
      ":\n",
      ";\n"
     ]
    },
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_url_query(s):\n",
    "    li = re.split('[=&]', urlparse(s)[4])\n",
    "    return [li[i] for i in range(len(li)) if i % 2 == 1]\n",
    "\n",
    "\n",
    "def find_max_str_length(x):\n",
    "    max_ = 0\n",
    "    li = [len(i) for i in x]\n",
    "    return max(li) if len(li) > 0 else 0\n",
    "\n",
    "\n",
    "def find_str_length_std(x):\n",
    "    max_ = 0\n",
    "    li = [len(i) for i in x]\n",
    "    return np.std(li) if len(li) > 0 else -1\n",
    "\n",
    "\n",
    "df['url_unquote'] = df['url'].apply(unquote)\n",
    "df['url_query'] = df['url_unquote'].apply(lambda x: get_url_query(x))\n",
    "df['url_query_num'] = df['url_query'].apply(len)\n",
    "df['url_query_max_len'] = df['url_query'].apply(find_max_str_length)\n",
    "df['url_query_len_std'] = df['url_query'].apply(find_str_length_std)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def find_url_filetype(x):\n",
    "    try:\n",
    "        return re.search(r'\\.[a-z]+', x).group()\n",
    "    except:\n",
    "        return '__NaN__'\n",
    "\n",
    "\n",
    "df['url_path'] = df['url_unquote'].apply(lambda x: urlparse(x)[2])\n",
    "df['url_filetype'] = df['url_path'].apply(lambda x: find_url_filetype(x))\n",
    "\n",
    "df['url_path_len'] = df['url_path'].apply(len)\n",
    "df['url_path_num'] = df['url_path'].apply(lambda x: len(re.findall('/', x)))\n",
    "\n",
    "# %%\n",
    "\n",
    "df['ua_short'] = df['user_agent'].apply(lambda x: x.split('/')[0])\n",
    "df['ua_first'] = df['user_agent'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "# %%\n",
    "\n",
    "# % % time\n",
    "\n",
    "\n",
    "def add_tfidf_feats(df, col, n_components=16):\n",
    "    text = list(df[col].values)\n",
    "    tf = TfidfVectorizer(min_df=1,\n",
    "                         analyzer='char_wb',\n",
    "                         ngram_range=(1, 3),\n",
    "                         stop_words='english')\n",
    "    tf.fit(text)\n",
    "    X = tf.transform(text)\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    svd.fit(X)\n",
    "    X_svd = svd.transform(X)\n",
    "    for i in range(n_components):\n",
    "        df[f'{col}_tfidf_{i}'] = X_svd[:, i]\n",
    "    return df\n",
    "\n",
    "def add_tfidf_feats_word(df, col, n_components=16):\n",
    "    text = list(df[col].values)\n",
    "    tf = TfidfVectorizer(min_df=1,\n",
    "                         analyzer='word',\n",
    "                         ngram_range=(1, 3),\n",
    "                         stop_words='english')\n",
    "    tf.fit(text)\n",
    "    X = tf.transform(text)\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    svd.fit(X)\n",
    "    X_svd = svd.transform(X)\n",
    "    for i in range(n_components):\n",
    "        df[f'{col}_tfidf_word_{i}'] = X_svd[:, i]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_tfidf_feats_char(df, col, n_components=16):\n",
    "    text = list(df[col].values)\n",
    "    tf = TfidfVectorizer(min_df=1,\n",
    "                         analyzer='char',\n",
    "                         ngram_range=(1, 3),\n",
    "                         stop_words='english')\n",
    "    tf.fit(text)\n",
    "    X = tf.transform(text)\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    svd.fit(X)\n",
    "    X_svd = svd.transform(X)\n",
    "    for i in range(n_components):\n",
    "        df[f'{col}_tfidf_char_{i}'] = X_svd[:, i]\n",
    "    return df\n",
    "\n",
    "df = add_tfidf_feats(df, 'url_unquote', n_components=256)\n",
    "df = add_tfidf_feats(df, 'user_agent', n_components=256)\n",
    "df = add_tfidf_feats(df, 'body', n_components=256)\n",
    "\n",
    "\n",
    "df = add_tfidf_feats_word(df, 'url_unquote', n_components=256)\n",
    "df = add_tfidf_feats_word(df, 'user_agent', n_components=256)\n",
    "df = add_tfidf_feats_word(df, 'body', n_components=256)\n",
    "\n",
    "df = add_tfidf_feats_char(df, 'url_unquote', n_components=256)\n",
    "df = add_tfidf_feats_char(df, 'user_agent', n_components=256)\n",
    "df = add_tfidf_feats_char(df, 'body', n_components=256)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "for col in tqdm(['method', 'refer', 'url_filetype', 'ua_short', 'ua_first']):\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# %%\n",
    "\n",
    "not_use_feats = ['id', 'user_agent', 'url', 'body', 'url_unquote', 'url_query', 'url_path', 'label']\n",
    "use_features = [col for col in df.columns if col not in not_use_feats]\n",
    "\n",
    "# %%\n",
    "\n",
    "train = df[df['label'].notna()]\n",
    "test = df[df['label'].isna()]\n",
    "\n",
    "train.shape, test.shape\n",
    "\n",
    "# %%\n",
    "\n",
    "NUM_CLASSES = 6\n",
    "FOLDS = 5\n",
    "TARGET = 'label'\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def run_lgb(df_train, df_test, use_features):\n",
    "    target = TARGET\n",
    "    oof_pred = np.zeros((len(df_train), NUM_CLASSES))\n",
    "    y_pred = np.zeros((len(df_test), NUM_CLASSES))\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=FOLDS)\n",
    "    for fold, (tr_ind, val_ind) in enumerate(folds.split(train, train[TARGET])):\n",
    "        print(f'Fold {fold + 1}')\n",
    "        x_train, x_val = df_train[use_features].iloc[tr_ind], df_train[use_features].iloc[val_ind]\n",
    "        y_train, y_val = df_train[target].iloc[tr_ind], df_train[target].iloc[val_ind]\n",
    "\n",
    "\n",
    "        params = {'random_seed': 2022}\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(x_train, y_train, eval_set=[(x_val, y_val)],\n",
    "                  verbose=10, eval_metric='auc')\n",
    "\n",
    "\n",
    "\n",
    "        oof_pred[val_ind] = model.predict_proba(x_val)\n",
    "        y_pred += model.predict_proba(df_test[use_features]) / folds.n_splits\n",
    "\n",
    "        #print(\"Features importance...\")\n",
    "        #gain = model.feature_importance('gain')\n",
    "        #feat_imp = pd.DataFrame({'feature': model.feature_name(),\n",
    "                                 #'split': model.feature_importance('split'),\n",
    "                                 #'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "        #print('Top 50 features:\\n', feat_imp.head(50))\n",
    "\n",
    "        del x_train, x_val, y_train, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    return y_pred, oof_pred\n",
    "\n",
    "\n",
    "y_pred, oof_pred = run_lgb(train, test, use_features)\n",
    "\n",
    "# %%\n",
    "\n",
    "print('acc: ', accuracy_score(np.argmax(oof_pred, axis=1), df_train['label']))\n",
    "\n",
    "# %%\n",
    "\n",
    "sub = pd.read_csv('../data/submit_example.csv')\n",
    "sub['predict'] = np.argmax(y_pred, axis=1)\n",
    "sub\n",
    "\n",
    "# %%\n",
    "\n",
    "sub['predict'].value_counts()\n",
    "\n",
    "# %%\n",
    "\n",
    "sub.to_csv('main_keyword_feats.csv', index=False)\n",
    "print('f1: ', f1_score(np.argmax(oof_pred, axis=1), df_train['label'], average='macro'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}